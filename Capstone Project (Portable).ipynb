{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install torchvision and kaggle\n",
    "!pip install torchvision\n",
    "!pip install kaggle\n",
    "!pip install tqdm\n",
    "!pip install colorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset and extract it\n",
    "! export KAGGLE_USERNAME=\"rakasyakti\" && export KAGGLE_KEY=\"492ba1715457e05fe0ba9184ca6ee1bf\" && kaggle datasets download --unzip ritikbompilwar/fishes-species-in-the-indian-subcontinent\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a gallery folder and moving one image from each class\n",
    "! mkdir StockFish-1/gallery\n",
    "! mv StockFish-1/test/Catla_100_jpg.rf.fcbb3bbf816aa35c0d0c94a1548a35a7.jpg StockFish-1/gallery/Catla.jpg\n",
    "! mv StockFish-1/test/Catla_100_jpg.rf.fcbb3bbf816aa35c0d0c94a1548a35a7.xml StockFish-1/gallery/Catla.xml\n",
    "! mv StockFish-1/test/CommonCarp_100_jpg.rf.517d664a011a7c09a036f9d46169eef5.jpg 'StockFish-1/gallery/Common Carp.jpg'\n",
    "! mv StockFish-1/test/CommonCarp_100_jpg.rf.517d664a011a7c09a036f9d46169eef5.xml 'StockFish-1/gallery/Common Carp.xml'\n",
    "! mv StockFish-1/test/Mori_106_jpg.rf.e06004d43b5052b143bc69fdaac03887.jpg StockFish-1/gallery/Mori.jpg\n",
    "! mv StockFish-1/test/Mori_106_jpg.rf.e06004d43b5052b143bc69fdaac03887.xml StockFish-1/gallery/Mori.xml\n",
    "! mv StockFish-1/test/Rohu_10_jpg.rf.e705e56159b5db86008f4ef08fa6660a.jpg StockFish-1/gallery/Rohu.jpg\n",
    "! mv StockFish-1/test/Rohu_10_jpg.rf.e705e56159b5db86008f4ef08fa6660a.xml StockFish-1/gallery/Rohu.xml\n",
    "! mv StockFish-1/test/SilverCarp_103_jpg.rf.40ea4daf9480dba4f2423feb4bc6c8f2.jpg 'StockFish-1/gallery/Silver Carp.jpg'\n",
    "! mv StockFish-1/test/SilverCarp_103_jpg.rf.40ea4daf9480dba4f2423feb4bc6c8f2.xml 'StockFish-1/gallery/Silver Carp.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the images in training folder into classes\n",
    "! mkdir StockFish-1/train/Catla\n",
    "! mv StockFish-1/train/Catla_* StockFish-1/train/Catla\n",
    "! mkdir 'StockFish-1/train/Common Carp'\n",
    "! mv StockFish-1/train/CommonCarp_* 'StockFish-1/train/Common Carp'\n",
    "! mkdir StockFish-1/train/Mori\n",
    "! mv StockFish-1/train/Mori_* StockFish-1/train/Mori\n",
    "! mkdir StockFish-1/train/Rohu\n",
    "! mv StockFish-1/train/Rohu_* StockFish-1/train/Rohu\n",
    "! mkdir 'StockFish-1/train/Silver Carp'\n",
    "! mv StockFish-1/train/SilverCarp* 'StockFish-1/train/Silver Carp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the images in validation folder into classes\n",
    "! mkdir StockFish-1/valid/Catla\n",
    "! mv StockFish-1/valid/Catla_* StockFish-1/valid/Catla\n",
    "! mkdir 'StockFish-1/valid/Common Carp'\n",
    "! mv StockFish-1/valid/CommonCarp_* 'StockFish-1/valid/Common Carp'\n",
    "! mkdir StockFish-1/valid/Mori\n",
    "! mv StockFish-1/valid/Mori_* StockFish-1/valid/Mori\n",
    "! mkdir StockFish-1/valid/Rohu\n",
    "! mv StockFish-1/valid/Rohu_* StockFish-1/valid/Rohu\n",
    "! mkdir 'StockFish-1/valid/Silver Carp'\n",
    "! mv StockFish-1/valid/SilverCarp_* 'StockFish-1/valid/Silver Carp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd && ls\n",
    "print(\"\\nFolders(classess) in training folder: ...\")\n",
    "! cd StockFish-1/train && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import  needed libraries and check the used gpu\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "from torchvision import transforms, models ,datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "ASL=np.array(glob.glob('StockFish-1/gallery/*.jpg')) \n",
    "\n",
    "fig = plt.figure(figsize=(30, 30))\n",
    "grid = ImageGrid(fig, 111, \n",
    "                 nrows_ncols=(1, 5),  \n",
    "                 axes_pad=0,  # pad between axes in inch.\n",
    "                 )\n",
    "l=0\n",
    "for img in ASL:\n",
    "        im=plt.imread(img)\n",
    "        grid[l].imshow(im,cmap='gray',interpolation='nearest')\n",
    "        grid[l].text(5,20, img.split('/')[2].split('.')[0] ,fontsize=30)\n",
    "        l+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the dataloaders \n",
    "\n",
    "# Define transforms for the training data and testing data\n",
    "train_path='StockFish-1/train'\n",
    "valid_path='StockFish-1/valid'\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.Resize((640,640)),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize((640,640)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data to loaders\n",
    "train_data = datasets.ImageFolder(train_path, transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(valid_path, transform=test_transforms)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=64)\n",
    "\n",
    "#print used Device\n",
    "print(f\"Device used: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "#print class to index mapping\n",
    "print(f\"class to index mapping: {train_data.class_to_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# choose a pretrained model to start with check options here: https://pytorch.org/vision/stable/models.html\n",
    "model = models.mobilenet_v2(weights='DEFAULT')\n",
    "\n",
    "# Freeze parameters of the tarined network \n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "#print the model to check the classifer and change it\n",
    "print (model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new classifier and append it to network but remember to have a 5-neuron output layer for our two classes.\n",
    "model.classifier= nn.Sequential(nn.Dropout(p=0.6, inplace=False),\n",
    "                                nn.Linear(in_features=1280, out_features=5, bias=True),\n",
    "                                nn.LogSoftmax(dim=1))\n",
    "\n",
    "# unlock last three blocks before the classifier(last layer).\n",
    "for p in model.features[-3:].parameters():\n",
    "    p.requires_grad = True  \n",
    "\n",
    "    \n",
    "# choose your loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# define optimizer to train only the classifier and the previous three block.\n",
    "optimizer = optim.Adam([{'params':model.features[-1].parameters()},\n",
    "                        {'params':model.features[-2].parameters()},\n",
    "                        {'params':model.features[-3].parameters()},\n",
    "                        {'params':model.classifier.parameters()}], lr=0.0005)\n",
    "\n",
    "# define Learning Rate scheduler to decrease the learning rate by multiplying it by 0.1 after each epoch on the data.\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "#print the classifier now\n",
    "print(model.classifier)\n",
    "\n",
    "#print the whole model\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### try your model on some images\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "#turn model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "#load some of the test data \n",
    "test_data_t = datasets.ImageFolder(valid_path,transforms.Compose([transforms.ToTensor()]))\n",
    "testloader_t = torch.utils.data.DataLoader(test_data_t, batch_size=50,shuffle=True)\n",
    "images_t , labels_t=next( iter(testloader_t) )\n",
    "\n",
    "#Choose arandom image from 0 to 50\n",
    "index = np.random.randint(0, 49)\n",
    "test_img=images_t[index]\n",
    "\n",
    "#show choosed image\n",
    "t=transforms.ToPILImage()\n",
    "plt.imshow(t(test_img))\n",
    "\n",
    "#normalize image as in the training data\n",
    "t_n=transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "test_img=t_n(test_img).unsqueeze(0).cuda()\n",
    "\n",
    "#classify image using our model\n",
    "res = torch.exp(model(test_img))\n",
    "\n",
    "#invert class_to_idx keys to values and viceversa.\n",
    "classes=train_data.class_to_idx\n",
    "classes = {value:key for key, value in classes.items()}\n",
    "\n",
    "print(f\"image number {index}\")\n",
    "print(\"---------------------\")\n",
    "\n",
    "#print real class\n",
    "print(\"label:\",classes[labels_t[index].item()])\n",
    "\n",
    "#print predicted class\n",
    "print(\"prediction:\", classes[res.argmax().item()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define number of epochs through data and run the training loop\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "epochs = 2\n",
    "step = 0\n",
    "running_loss = 0\n",
    "print_every = 2\n",
    "trainlossarr=[]\n",
    "testlossarr=[]\n",
    "oldacc=0\n",
    "\n",
    "steps=math.ceil(len(train_data)/(trainloader.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore,Style\n",
    "import sys\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(Style.RESET_ALL)\n",
    "    print(f\"--------------------------------- START OF EPOCH [ {epoch+1} ] >>> LR =  {optimizer.param_groups[-1]['lr']} ---------------------------------\\n\")\n",
    "    for inputs, labels in tqdm(trainloader,desc=Fore.GREEN +f\"* PROGRESS IN EPOCH {epoch+1} \",file=sys.stdout):\n",
    "        model.train()\n",
    "        step += 1\n",
    "        inputs=inputs.to(device)\n",
    "        labels=labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        props = model.forward(inputs)\n",
    "        loss = criterion(props, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (step % print_every == 0) or (step==steps):\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in testloader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    props = model.forward(inputs)\n",
    "                    batch_loss = criterion(props, labels)\n",
    "\n",
    "                    test_loss += batch_loss.item()\n",
    "\n",
    "                    # Calculate accuracy\n",
    "                    ps = torch.exp(props)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    " \n",
    "                    \n",
    "                        \n",
    "\n",
    "            tqdm.write(f\"Epoch ({epoch+1} of {epochs}) ... \"\n",
    "                  f\"Step  ({step:3d} of {steps}) ... \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f} ... \"\n",
    "                  f\"Test loss: {test_loss/len(testloader):.3f} ... \"\n",
    "                  f\"Test accuracy: {accuracy/len(testloader):.3f} \")\n",
    "            trainlossarr.append(running_loss/print_every)\n",
    "            testlossarr.append(test_loss/len(testloader))\n",
    "            running_loss = 0\n",
    "            \n",
    "        \n",
    "    scheduler.step()\n",
    "    step=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### try your model on some images\n",
    "%matplotlib inline\n",
    "\n",
    "#turn model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "#load some of the test data \n",
    "test_data = datasets.ImageFolder(valid_path,transforms.Compose([transforms.ToTensor()]))\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=50,shuffle=True)\n",
    "images , labels=next( iter(testloader) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose arandom image from 0 to 49\n",
    "index = np.random.randint(0, 49)\n",
    "test_img=images[index]\n",
    "\n",
    "#show choosed image\n",
    "t=transforms.ToPILImage()\n",
    "plt.imshow(t(test_img))\n",
    "\n",
    "#normalize image as in the training data\n",
    "t_n=transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "test_img=t_n(test_img).unsqueeze(0).cuda()\n",
    "\n",
    "#classify image using our model\n",
    "res = torch.exp(model(test_img))\n",
    "\n",
    "#invert class_to_idx keys to values and viceversa.\n",
    "classes=train_data.class_to_idx\n",
    "classes = {value:key for key, value in classes.items()}\n",
    "\n",
    "print(f\"image number {index}\")\n",
    "print(\"---------------------\")\n",
    "\n",
    "#print real class\n",
    "print(\"label:\",classes[labels[index].item()])\n",
    "\n",
    "#print predicted class\n",
    "print(\"prediction:\", classes[res.argmax().item()])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
